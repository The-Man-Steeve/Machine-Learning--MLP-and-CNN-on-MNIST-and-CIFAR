{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "ZYpI5u2Qe8bb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EWMyNUvaESRy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets\n",
        "import torch.nn.functional as f\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we define the architecture for the networks"
      ],
      "metadata": {
        "id": "L0deYqqRe1HD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class shallowNet(nn.Module):\n",
        "  #Structure:\n",
        "        #---Input layer (784 nodes)\n",
        "        #---1 Hidden Layer (128 nodes)\n",
        "        #---Output Layer (10 nodes)\n",
        "    def __init__(self, input_features=784, h1=128, output_features=10, dropout_rate = 0):\n",
        "      super().__init__()\n",
        "      self.flatten = nn.Flatten()\n",
        "      self.fc1 = nn.Linear(input_features, h1)\n",
        "      self.out = nn.Linear(h1, output_features)\n",
        "      self.dropout = nn.Dropout(dropout_rate) # Initialize dropout layer\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.flatten(x) # Add flatten here\n",
        "      x = f.relu(self.fc1(x))\n",
        "      x = self.dropout(x) # Use the initialized dropout layer\n",
        "      x = self.out(x)\n",
        "      return x"
      ],
      "metadata": {
        "id": "ew1yordmEtI9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class mediumNet(nn.Module):\n",
        "  '''\n",
        "  Structure:\n",
        "  Input layer (784 nodes)\n",
        "  1st Hidden layer: 512 nodes\n",
        "  2nd Hidden layer: 256 nodes\n",
        "  3rd Hidden layer: 128 nodes\n",
        "  Output Layer (10 nodes)\n",
        "  '''\n",
        "  def __init__(self, input_features=784, h1=512, h2=256, h3=128, output_features=10, dropout_rate= 0):\n",
        "    super().__init__()\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.fc1 = nn.Linear(input_features, h1)\n",
        "    self.fc2 = nn.Linear(h1,h2)\n",
        "    self.fc3 = nn.Linear(h2,h3)\n",
        "    self.out = nn.Linear(h3, output_features)\n",
        "    self.dropout = nn.Dropout(dropout_rate) # Initialize dropout layer\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.flatten(x) # Add flatten here\n",
        "    x = f.relu(self.fc1(x))\n",
        "    x = self.dropout(x)\n",
        "    x = f.relu(self.fc2(x))\n",
        "    x = self.dropout(x)\n",
        "    x = f.relu(self.fc3(x))\n",
        "    x = self.dropout(x)\n",
        "    x = self.out(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "yDbWzOOvw3ty"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class deepNet(nn.Module):\n",
        "  def __init__(self, input_features=784, h1=1024, h2=512, h3=256, h4=512, h5=128, output_features=10, dropout_rate=0):\n",
        "    super().__init__()\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.fc1 = nn.Linear(input_features, h1)\n",
        "    self.fc2 = nn.Linear(h1, h2)\n",
        "    self.fc3 = nn.Linear(h2, h3)\n",
        "    self.fc4 = nn.Linear(h3, h4)\n",
        "    self.fc5 = nn.Linear(h4, h5)\n",
        "    self.out = nn.Linear(h5, output_features)\n",
        "    self.dropout = nn.Dropout(dropout_rate) # Initialize dropout layer\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.flatten(x) # Add flatten here\n",
        "    x = f.relu(self.fc1(x))\n",
        "    x = self.dropout(x)\n",
        "    x = f.relu(self.fc2(x))\n",
        "    x = self.dropout(x)\n",
        "    x = f.relu(self.fc3(x))\n",
        "    x = self.dropout(x)\n",
        "    x = f.relu(self.fc4(x))\n",
        "    x = self.dropout(x)\n",
        "    x = f.relu(self.fc5(x))\n",
        "    x = self.dropout(x)\n",
        "    x = self.out(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "Avos3-AKw59N"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we define the Architecture for the CNNs"
      ],
      "metadata": {
        "id": "DMZJjrnjIP6s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Baseline**:\n",
        " Cifar -> 64 * 16 * 16\n",
        " MNIST -> 64 * 14 * 14\n",
        "\n",
        "**Enhanced**:\n",
        "cifar -> 64 * 16 * 16\n",
        "MNIST -> 64 * 14 * 14\n",
        "\n",
        "**Deep**\n",
        "cifar -> 256 * 8 * 8\n",
        "MNIST -> 256 * 14 * 14"
      ],
      "metadata": {
        "id": "tYzW-985jNE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class baseline_CNN(nn.Module):\n",
        "  def __init__(self, input_channels=1):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels=input_channels, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "    self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "    self.pool = nn.MaxPool2d(kernel_size=2)\n",
        "    if input_channels == 1: #if its MNIST\n",
        "      self.fc = nn.Linear(64 * 14 * 14, 10)\n",
        "    else:\n",
        "      self.fc = nn.Linear(64 * 16 * 16, 10) # Updated from 64 * 14 * 14 to 64 * 16 * 16 for 32x32 input\n",
        "    self.flatten = nn.Flatten()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = f.relu(x)\n",
        "    x = self.conv2(x)\n",
        "    x = f.relu(x)\n",
        "    x = self.pool(x)\n",
        "    x = self.flatten(x)\n",
        "    x = self.fc(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "9tIpgeqACPfm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class enhanced_CNN(nn.Module):\n",
        "  def __init__(self, dropout_rate=0.2, input_channels=1):\n",
        "    super().__init__()\n",
        "    #convolutional block\n",
        "    #adds batch normalization and dropout\n",
        "    self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, stride=1, padding=1) # Changed from 1 to input_channels\n",
        "    self.batchnorm1 = nn.BatchNorm2d(32)\n",
        "    self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "    self.batchnorm2 = nn.BatchNorm2d(64)\n",
        "    self.pool = nn.MaxPool2d(kernel_size=2)\n",
        "    #fully connected block\n",
        "    if input_channels == 1: #if its MNIST\n",
        "      self.fc1 = nn.Linear(64 * 14 * 14, 10)\n",
        "    else:\n",
        "      self.fc1 = nn.Linear(64 * 16 * 16, 10)\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "    self.flatten = nn.Flatten()\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.batchnorm1(x)\n",
        "    x = f.relu(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.batchnorm2(x)\n",
        "    x = f.relu(x)\n",
        "    x = self.pool(x)\n",
        "    x = self.flatten(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.fc1(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "L-cd4NFxITWM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class deep_CNN(nn.Module):\n",
        "  def __init__(self, dropout_rate=0.2, input_channels=1):\n",
        "    super().__init__()\n",
        "    #pass through first conv layer\n",
        "    self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, stride=1, padding=1) # Changed from 1 to input_channels\n",
        "    self.batchNorm1 = nn.BatchNorm2d(32)\n",
        "    #pass through 2nd conv layer\n",
        "    self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "    self.batchNorm2 = nn.BatchNorm2d(64)\n",
        "    #apply pooling\n",
        "    self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "    #pass through third conv layer\n",
        "    self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "    self.batchNorm3 = nn.BatchNorm2d(128)\n",
        "    #Pass through a fourth convolutional layer\n",
        "    self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "    self.batchNorm4 = nn.BatchNorm2d(256)\n",
        "    #pool one last time\n",
        "    self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "    #flatten\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "    if input_channels == 1:\n",
        "      self.fc1 = nn.Linear(256 * 14 * 14, 10)\n",
        "    else:\n",
        "      self.fc1 = nn.Linear(256 * 16 * 16, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    #1st convolution\n",
        "    x = self.conv1(x)\n",
        "    x = self.batchNorm1(x)\n",
        "    x = f.relu(x)\n",
        "    #2nd convolution\n",
        "    x = self.conv2(x)\n",
        "    x = self.batchNorm2(x)\n",
        "    x = f.relu(x)\n",
        "    #1st pool\n",
        "    x = self.pool1(x)\n",
        "    #3rd convolution\n",
        "    x = self.conv3(x)\n",
        "    x = self.batchNorm3(x)\n",
        "    x = f.relu(x)\n",
        "    #4th convolution\n",
        "    x = self.conv4(x)\n",
        "    x = self.batchNorm4(x)\n",
        "    x = f.relu(x)\n",
        "    #pass into fully connected layer\n",
        "    x = self.flatten(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.fc1(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "G1fxisCgIYbo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Here we Load in the data"
      ],
      "metadata": {
        "id": "GDyoCuSrfn_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
        "cifar_train = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "cifar_test = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "mnist_train, mnist_val = torch.utils.data.random_split(mnist_train, [50000, 10000], generator=torch.Generator().manual_seed(42))\n",
        "cifar_train, cifar_val = torch.utils.data.random_split(cifar_train, [45000,5000], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "#print(mnist_x_train)\n",
        "#print(mnist_y_train)"
      ],
      "metadata": {
        "id": "5Kf4lMc9cQrp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "551c60ec-096a-4bc1-dcd5-a2fc3597d77d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 12.6MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 334kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 3.20MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 12.3MB/s]\n",
            "100%|██████████| 170M/170M [00:05<00:00, 29.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experiment**\n",
        "\n",
        "This function takes in all the input parameters and trains any specified model on any specified dataset and returns the model itself as well as its validation accuracy"
      ],
      "metadata": {
        "id": "GORqKrk3i_wZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#actual Experiment\n",
        "#import matplotlib.pyplot as plt #plot for debugging\n",
        "from torch.utils.data import DataLoader #to load in and iterate through the data\n",
        "\n",
        "def experiment(model_type, data_set, learning_rate, batch_size, optimizer, dropout_rate):\n",
        "  # Define the device\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#=================================================================================\n",
        "# Data Loaders and Batch size\n",
        "  if data_set == 'mnist':\n",
        "    train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, generator=torch.Generator().manual_seed(42))\n",
        "    val_loader = DataLoader(mnist_val, batch_size=batch_size, shuffle=False, generator=torch.Generator().manual_seed(42))\n",
        "    inputs = 784\n",
        "    channels = 1\n",
        "  elif data_set == 'cifar':\n",
        "    train_loader = DataLoader(cifar_train, batch_size=batch_size, shuffle=True, generator=torch.Generator().manual_seed(42))\n",
        "    val_loader = DataLoader(cifar_val, batch_size=batch_size, shuffle=False, generator=torch.Generator().manual_seed(42))\n",
        "    inputs = 3072\n",
        "    channels = 3\n",
        "  #Model\n",
        "  if model_type == 'shallow':\n",
        "    model = shallowNet(dropout_rate=dropout_rate, input_features=inputs).to(device)\n",
        "  elif model_type == 'medium':\n",
        "    model = mediumNet(dropout_rate=dropout_rate, input_features=inputs).to(device)\n",
        "  elif model_type == 'deep':\n",
        "    model = deepNet(dropout_rate=dropout_rate, input_features=inputs).to(device)\n",
        "  elif model_type == 'baseline':\n",
        "    model = (baseline_CNN(input_channels=channels)).to(device) # Pass input_channels\n",
        "  elif model_type == 'enhanced':\n",
        "    model = enhanced_CNN(dropout_rate=dropout_rate, input_channels=channels).to(device) # Pass input_channels\n",
        "  elif model_type == 'deep_cnn':\n",
        "    model = deep_CNN(dropout_rate=dropout_rate, input_channels=channels).to(device) # Pass input_channels\n",
        "#Optimizer\n",
        "  criterion = nn.CrossEntropyLoss() #set the criterion\n",
        "  if optimizer == 'adam':\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "  else:\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "  #=========================================================================\n",
        "#actual Experiment\n",
        "  #print(\"Running experiment with model: \", model_type, \" and dataset: \", data_set)\n",
        "  epochs = 10\n",
        "  accuracy_val = []\n",
        "  #main loop\n",
        "  for epoch in range(epochs):\n",
        "    model.train() #set the model to training mode\n",
        "    for x, y in train_loader:\n",
        "      x, y = x.to(device), y.to(device) # Send data to device\n",
        "      y_pred = model.forward(x) #run the forward pass\n",
        "      loss = criterion(y_pred, y) #evaluate the loss function\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward() #compute gradient\n",
        "      optimizer.step() #bagpropagation step\n",
        "#====================================================================================\n",
        "  #now that the model is trained, we evaluate on the validation set\n",
        "  model.eval() #set the model to evaluation mode\n",
        "  total_predictions = 0\n",
        "  correct_predictions = 0\n",
        "  with torch.no_grad(): # Add no_grad for validation\n",
        "    for x, y in val_loader:\n",
        "      x, y = x.to(device), y.to(device) # Send data to device\n",
        "      #get the accuracy for the epoch\n",
        "      y_pred = model.forward(x) #run the validation set on through the model\n",
        "      prediction = torch.argmax(y_pred, dim=1) #get the predictions from the model for each sample\n",
        "      #record predictions to report accuracy\n",
        "      for i in range(len(prediction)):\n",
        "        if prediction[i] == y[i]:\n",
        "          correct_predictions += 1\n",
        "        total_predictions += 1\n",
        "    #compute the final accuracy\n",
        "    acc = (100.0 * (correct_predictions/total_predictions))\n",
        "    #print(acc)\n",
        "    return [model, acc]\n",
        "\n",
        "  #plt.plot(loss_train, label='Training Loss') # loss_train is not defined\n",
        "  #plt.plot(loss_val, label='Validation Loss') # These plotting lines are commented out\n",
        "  #plt.plot(accuracy_val, label='Validation Accuracy') # These plotting lines are commented out\n",
        "  #plt.xlabel('Epoch')\n",
        "  #plt.ylabel('Loss/Accuracy')\n",
        "  #plt.ylim(0,100)\n",
        "  #plt.legend()\n",
        "  #plt.show()"
      ],
      "metadata": {
        "id": "lHzdEFVCjA-Z",
        "collapsed": true
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**run_experiments**\n",
        "\n",
        "This method does hyperparameter tuning for a specific model type on a specific dataset and returns the best model with its accuracy as well as the runtime for that specific model\n",
        "\n",
        "**NOTE**\n",
        "\n",
        "The runtime returned is for the specific model that is best, not the overall runtime of all the models trained"
      ],
      "metadata": {
        "id": "LjV39gxKf_Hz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "def run_experiments(model_type, data_set):\n",
        "  lr = [0.001]\n",
        "  batch_size = [32, 64, 128]\n",
        "  optimizer = ['adam']\n",
        "  dropout_rate = [0.0, 0.2, 0.5]\n",
        "\n",
        "  best_candidate = None\n",
        "  best_accuracy = 0.0\n",
        "  best_config = None\n",
        "\n",
        "  for l in lr:\n",
        "    for b in batch_size:\n",
        "      for o in optimizer:\n",
        "        for d in dropout_rate:\n",
        "          start_time = time.time()\n",
        "          #print(f\"Testing with learning rate: {l}, batch size: {b}, optimizer: {o}, dropout rate: {d}\")\n",
        "          candidate, candidate_acc = experiment(model_type, data_set, l, b, o, d)\n",
        "          end_time = time.time()\n",
        "          if candidate_acc > best_accuracy:\n",
        "            best_candidate = candidate\n",
        "            best_accuracy = candidate_acc\n",
        "            best_batch_size = b\n",
        "            best_config = {'lr': l, 'batch_size': b, 'optimizer': o, 'dropout_rate': d}\n",
        "            best_runtime = (end_time - start_time) / 60.0\n",
        "  return best_candidate, best_accuracy, best_config, best_runtime\n",
        "#=================================================================================================================="
      ],
      "metadata": {
        "id": "jVJYdg5J85Tt"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Script to run MNIST all MLP architectures"
      ],
      "metadata": {
        "id": "hUrd8OMlne49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_candidate = None\n",
        "best_accuracy = 0.0\n",
        "best_config = None\n",
        "\n",
        "#finding hyperparameters for Shallow Architecture\n",
        "print(\"Working on Shallow Architecture... \")\n",
        "shallow_candidate, shallow_accuracy, shallow_config, shallow_runtime = run_experiments('shallow', 'mnist')\n",
        "print(\"Best candidate: \", shallow_config)\n",
        "print(\"Validation Accuracy: \", shallow_accuracy)\n",
        "print(\"Runtime: \", shallow_runtime, \"Min\")\n",
        "best_accuracy = shallow_accuracy\n",
        "best_candidate = shallow_candidate\n",
        "best_config = shallow_config\n",
        "\n",
        "#tuning hyperparameters for medium architecture\n",
        "print(\"Working on Medium Architecture... \")\n",
        "medium_candidate, medium_accuracy, medium_config, medium_runtime = run_experiments('medium', 'mnist')\n",
        "print(\"Best Candidate: \", medium_config)\n",
        "print(\"Validation Accuracy: \", medium_accuracy)\n",
        "print(\"Runtime: \", medium_runtime, \"Min\")\n",
        "if medium_accuracy > best_accuracy:\n",
        "  best_candidate = medium_candidate\n",
        "  best_accuracy = medium_accuracy\n",
        "  best_config = medium_config\n",
        "\n",
        "#tuning hyperparameters for deep architecture\n",
        "print(\"Working on Deep Architecture...\")\n",
        "deep_candidate, deep_accuracy, deep_config, deep_runtime = run_experiments('deep', 'mnist')\n",
        "print(\"Best Deep Candidate: \", deep_config)\n",
        "print(\"Validation Accuracy: \", deep_accuracy)\n",
        "print(f'Runtime: ', deep_runtime,   'Min')\n",
        "if deep_accuracy > best_accuracy:\n",
        "  best_candidate = deep_candidate\n",
        "  best_accuracy = deep_accuracy\n",
        "  best_config = deep_config\n",
        "\n",
        "print(\"Best candidate: \", best_config)\n",
        "\n",
        "print(\"Testing on the test Set: \")\n",
        "test_loader = DataLoader(mnist_test, shuffle=False, generator=torch.Generator().manual_seed(42))\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Define device for testing\n",
        "best_candidate.to(device) # Send model to device for testing\n",
        "best_candidate.eval() #evaluation mode\n",
        "total_predictions = 0\n",
        "correct_predictions = 0\n",
        "with torch.no_grad():\n",
        "  for x, y in test_loader:\n",
        "    x, y = x.to(device), y.to(device) # Send data to device for testing\n",
        "    y_pred = best_candidate.forward(x)\n",
        "    prediction = torch.argmax(y_pred, dim=1) #prediction is the largest output\n",
        "    for i in range(len(prediction)):\n",
        "      if prediction[i] == y[i]:\n",
        "        correct_predictions += 1\n",
        "      total_predictions += 1\n",
        "acc = (100.0 * (correct_predictions/total_predictions))\n",
        "print(\"Test Accuracy on Final MLP Model: \", acc)"
      ],
      "metadata": {
        "id": "ynrDE0zyKj0l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "039eea25-da35-4ba0-a48b-dacd3abae9e5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Working on Shallow Architecture... \n",
            "Best candidate:  {'lr': 0.001, 'batch_size': 32, 'optimizer': 'adam', 'dropout_rate': 0.0}\n",
            "Validation Accuracy:  97.61\n",
            "Runtime:  1.2985139568646749 Min\n",
            "Working on Medium Architecture... \n",
            "Best Candidate:  {'lr': 0.001, 'batch_size': 128, 'optimizer': 'adam', 'dropout_rate': 0.2}\n",
            "Validation Accuracy:  97.87\n",
            "Runtime:  1.0283886631329855 Min\n",
            "Working on Deep Architecture...\n",
            "Best Deep Candidate:  {'lr': 0.001, 'batch_size': 128, 'optimizer': 'adam', 'dropout_rate': 0.5}\n",
            "Validation Accuracy:  97.87\n",
            "Runtime:  1.0498501539230347 Min\n",
            "Best candidate:  {'lr': 0.001, 'batch_size': 128, 'optimizer': 'adam', 'dropout_rate': 0.2}\n",
            "Testing on the test Set: \n",
            "Test Accuracy on Final MLP Model:  98.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Script to test MLP Architectures on CIFAR"
      ],
      "metadata": {
        "id": "I8MPTZlCum5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_candidate = None\n",
        "best_accuracy = 0.0\n",
        "best_config = None\n",
        "\n",
        "#finding hyperparameters for Shallow Architecture\n",
        "print(\"Working on Shallow Architecture... \")\n",
        "shallow_candidate, shallow_accuracy, shallow_config, shallow_runtime = run_experiments('shallow', 'cifar')\n",
        "print(\"Best candidate: \", shallow_config)\n",
        "print(\"Validation Accuracy: \", shallow_accuracy)\n",
        "print(\"Runtime: \", shallow_runtime, \" Min\")\n",
        "best_accuracy = shallow_accuracy\n",
        "best_candidate = shallow_candidate\n",
        "best_config = shallow_config\n",
        "\n",
        "#tuning hyperparameters for medium architecture\n",
        "print(\"Working on Medium Architecture... \")\n",
        "medium_candidate, medium_accuracy, medium_config, medium_runtime = run_experiments('medium', 'cifar')\n",
        "print(\"Best Candidate: \", medium_config)\n",
        "print(\"Validation Accuracy: \", medium_accuracy)\n",
        "print(\"Runtime: \", medium_runtime, \" Min\")\n",
        "if medium_accuracy > best_accuracy:\n",
        "  best_candidate = medium_candidate\n",
        "  best_accuracy = medium_accuracy\n",
        "  best_config = medium_config\n",
        "\n",
        "#tuning hyperparameters for deep architecture\n",
        "print(\"Working on Deep Architecture...\")\n",
        "deep_candidate, deep_accuracy, deep_config, deep_runtime = run_experiments('deep', 'cifar')\n",
        "print(\"Best Candidate: \", deep_config)\n",
        "print(\"Validation Accuracy: \", deep_accuracy)\n",
        "print('Runtime: ', deep_runtime,   ' Min')\n",
        "if deep_accuracy > best_accuracy:\n",
        "  best_candidate = deep_candidate\n",
        "  best_accuracy = deep_accuracy\n",
        "  best_config = deep_config\n",
        "\n",
        "print(\"Best candidate: \", best_config)\n",
        "\n",
        "print(\"Testing on the test Set: \")\n",
        "test_loader = DataLoader(cifar_test, shuffle=False, generator=torch.Generator().manual_seed(42))\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Define device for testing\n",
        "best_candidate.to(device) # Send model to device for testing\n",
        "best_candidate.eval() #evaluation mode\n",
        "total_predictions = 0\n",
        "correct_predictions = 0\n",
        "with torch.no_grad():\n",
        "  for x, y in test_loader:\n",
        "    x, y = x.to(device), y.to(device) # Send data to device for testing\n",
        "    y_pred = best_candidate.forward(x)\n",
        "    prediction = torch.argmax(y_pred, dim=1) #prediction is the largest output\n",
        "    for i in range(len(prediction)):\n",
        "      if prediction[i] == y[i]:\n",
        "        correct_predictions += 1\n",
        "      total_predictions += 1\n",
        "acc = (100.0 * (correct_predictions/total_predictions))\n",
        "print(\"Test Accuracy on Final MLP Model: \", acc)"
      ],
      "metadata": {
        "id": "ujFLLuwwupie",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb2ec359-fe94-4213-9e0d-629d475572c7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Working on Shallow Architecture... \n",
            "Best candidate:  {'lr': 0.001, 'batch_size': 64, 'optimizer': 'adam', 'dropout_rate': 0.0}\n",
            "Validation Accuracy:  46.339999999999996\n",
            "Runtime:  1.1633050839106243  Min\n",
            "Working on Medium Architecture... \n",
            "Best Candidate:  {'lr': 0.001, 'batch_size': 128, 'optimizer': 'adam', 'dropout_rate': 0.0}\n",
            "Validation Accuracy:  49.120000000000005\n",
            "Runtime:  1.0917295734087626  Min\n",
            "Working on Deep Architecture...\n",
            "Best Candidate:  {'lr': 0.001, 'batch_size': 64, 'optimizer': 'adam', 'dropout_rate': 0.0}\n",
            "Validation Accuracy:  48.14\n",
            "Runtime:  1.2163305242856344  Min\n",
            "Best candidate:  {'lr': 0.001, 'batch_size': 128, 'optimizer': 'adam', 'dropout_rate': 0.0}\n",
            "Testing on the test Set: \n",
            "Test Accuracy on Final MLP Model:  50.33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN Architectures for MNIST"
      ],
      "metadata": {
        "id": "CCwGTqZUnbuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_candidate = None\n",
        "best_accuracy = 0.0\n",
        "best_config = None\n",
        "\n",
        "#finding hyperparameters for Baseline Architecture\n",
        "print(\"Working on Baseline Architecture... \")\n",
        "Baseline_candidate, Baseline_accuracy, Baseline_config, Baseline_runtime = run_experiments('baseline', 'mnist')\n",
        "print(\"Best candidate: \", Baseline_config)\n",
        "print(\"Validation Accuracy: \", Baseline_accuracy)\n",
        "print(\"Runtime: \", Baseline_runtime, \"Min\")\n",
        "best_accuracy = Baseline_accuracy\n",
        "best_candidate = Baseline_candidate\n",
        "best_config = Baseline_config\n",
        "\n",
        "#tuning hyperparameters for medium architecture\n",
        "print(\"Working on Enhanced Architecture... \")\n",
        "enhanced_candidate, enhanced_accuracy, enhanced_config, enhanced_runtime = run_experiments('enhanced', 'mnist')\n",
        "print(\"Runtime: \", enhanced_runtime, \" Min\")\n",
        "print(\"Validation Accuracy:\" , enhanced_accuracy)\n",
        "print(\"Best Candidate: \", enhanced_config)\n",
        "if enhanced_accuracy > best_accuracy:\n",
        "  best_candidate = enhanced_candidate\n",
        "  best_accuracy = enhanced_accuracy\n",
        "  best_config = enhanced_config\n",
        "\n",
        "#tuning hyperparameters for deep architecture\n",
        "print(\"Working on Deep Architecture...\")\n",
        "deep_candidate, deep_accuracy, deep_config, deep_runtime = run_experiments('deep_cnn', 'mnist')\n",
        "print(\"Best Deep Candidate: \", deep_config)\n",
        "print(\"Validation Accuracy: \", deep_accuracy)\n",
        "print(f'Runtime: ', deep_runtime,   ' Min')\n",
        "if deep_accuracy > best_accuracy:\n",
        "  best_candidate = deep_candidate\n",
        "  best_accuracy = deep_accuracy\n",
        "  best_config = deep_config\n",
        "\n",
        "print(\"Best candidate: \", best_config)\n",
        "\n",
        "print(\"Testing on the test Set: \")\n",
        "test_loader = DataLoader(mnist_test, shuffle=False, generator=torch.Generator().manual_seed(42))\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Define device for testing\n",
        "best_candidate.to(device) # Send model to device for testing\n",
        "best_candidate.eval() #evaluation mode\n",
        "total_predictions = 0\n",
        "correct_predictions = 0\n",
        "with torch.no_grad():\n",
        "  for x, y in test_loader:\n",
        "    x, y = x.to(device), y.to(device) # Send data to device for testing\n",
        "    y_pred = best_candidate.forward(x)\n",
        "    prediction = torch.argmax(y_pred, dim=1) #prediction is the largest output\n",
        "    for i in range(len(prediction)):\n",
        "      if prediction[i] == y[i]:\n",
        "        correct_predictions += 1\n",
        "      total_predictions += 1\n",
        "acc = (100.0 * (correct_predictions/total_predictions))\n",
        "print(\"Test Accuracy on Final MLP Model: \", acc)"
      ],
      "metadata": {
        "id": "kWMjvZ4mEWRh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b044c3c-5b8f-440d-a172-36ffe96ca844"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Working on Baseline Architecture... \n",
            "Best candidate:  {'lr': 0.001, 'batch_size': 32, 'optimizer': 'adam', 'dropout_rate': 0.2}\n",
            "Validation Accuracy:  98.65\n",
            "Runtime:  1.4425546646118164 Min\n",
            "Working on Enhanced Architecture... \n",
            "Runtime:  1.5719817479451497  Min\n",
            "Validation Accuracy: 98.78\n",
            "Best Candidate:  {'lr': 0.001, 'batch_size': 32, 'optimizer': 'adam', 'dropout_rate': 0.2}\n",
            "Working on Deep Architecture...\n",
            "Best Deep Candidate:  {'lr': 0.001, 'batch_size': 32, 'optimizer': 'adam', 'dropout_rate': 0.5}\n",
            "Validation Accuracy:  99.19\n",
            "Runtime:  1.7923338770866395  Min\n",
            "Best candidate:  {'lr': 0.001, 'batch_size': 32, 'optimizer': 'adam', 'dropout_rate': 0.5}\n",
            "Testing on the test Set: \n",
            "Test Accuracy on Final MLP Model:  99.31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN Architectures for CIFAR Dataset"
      ],
      "metadata": {
        "id": "OiQwTL34HAsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_candidate = None\n",
        "best_accuracy = 0.0\n",
        "best_config = None\n",
        "\n",
        "#finding hyperparameters for Baseline Architecture\n",
        "print(\"Working on Baseline Architecture... \")\n",
        "Baseline_candidate, Baseline_accuracy, Baseline_config, Baseline_runtime = run_experiments('baseline', 'cifar')\n",
        "print(\"Best candidate: \", Baseline_config)\n",
        "print(\"Validation Accuracy: \", Baseline_accuracy)\n",
        "print(\"Runtime: \", Baseline_runtime, \"Min\")\n",
        "best_accuracy = Baseline_accuracy\n",
        "best_candidate = Baseline_candidate\n",
        "best_config = Baseline_config\n",
        "\n",
        "#tuning hyperparameters for medium architecture\n",
        "print(\"Working on Enhanced Architecture... \")\n",
        "enhanced_candidate, enhanced_accuracy, enhanced_config, enhanced_runtime = run_experiments('enhanced', 'cifar')\n",
        "print(\"Validation Accuracy:\" , enhanced_accuracy)\n",
        "print(\"Best Candidate: \", enhanced_config)\n",
        "print(\"Runtime: \", enhanced_runtime, \" Min\")\n",
        "if enhanced_accuracy > best_accuracy:\n",
        "  best_candidate = enhanced_candidate\n",
        "  best_accuracy = enhanced_accuracy\n",
        "  best_config = enhanced_config\n",
        "\n",
        "#tuning hyperparameters for deep architecture\n",
        "print(\"Working on Deep Architecture...\")\n",
        "deep_candidate, deep_accuracy, deep_config, deep_runtime = run_experiments('deep_cnn', 'cifar')\n",
        "print(\"Best Deep Candidate: \", deep_config)\n",
        "print(\"Validation Accuracy: \", deep_accuracy)\n",
        "print(f'Runtime: ', deep_runtime,   ' Min')\n",
        "if deep_accuracy > best_accuracy:\n",
        "  best_candidate = deep_candidate\n",
        "  best_accuracy = deep_accuracy\n",
        "  best_config = deep_config\n",
        "\n",
        "print(\"Best candidate: \", best_config)\n",
        "\n",
        "print(\"Testing on the test Set: \")\n",
        "test_loader = DataLoader(cifar_test, shuffle=False, generator=torch.Generator().manual_seed(42))\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Define device for testing\n",
        "best_candidate.to(device) # Send model to device for testing\n",
        "best_candidate.eval() #evaluation mode\n",
        "total_predictions = 0\n",
        "correct_predictions = 0\n",
        "with torch.no_grad():\n",
        "  for x, y in test_loader:\n",
        "    x, y = x.to(device), y.to(device) # Send data to device for testing\n",
        "    y_pred = best_candidate.forward(x)\n",
        "    prediction = torch.argmax(y_pred, dim=1) #prediction is the largest output\n",
        "    for i in range(len(prediction)):\n",
        "      if prediction[i] == y[i]:\n",
        "        correct_predictions += 1\n",
        "      total_predictions += 1\n",
        "acc = (100.0 * (correct_predictions/total_predictions))\n",
        "print(\"Test Accuracy on Final MLP Model: \", acc)"
      ],
      "metadata": {
        "id": "IgDHxzkXHEpd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86c2d3a3-d17a-481a-f7c5-f71052003aca"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Working on Baseline Architecture... \n",
            "Best candidate:  {'lr': 0.001, 'batch_size': 64, 'optimizer': 'adam', 'dropout_rate': 0.0}\n",
            "Validation Accuracy:  67.9\n",
            "Runtime:  1.2014027078946432 Min\n",
            "Working on Enhanced Architecture... \n",
            "Validation Accuracy: 69.92\n",
            "Best Candidate:  {'lr': 0.001, 'batch_size': 32, 'optimizer': 'adam', 'dropout_rate': 0.2}\n",
            "Runtime:  1.541283110777537  Min\n",
            "Working on Deep Architecture...\n",
            "Best Deep Candidate:  {'lr': 0.001, 'batch_size': 32, 'optimizer': 'adam', 'dropout_rate': 0.5}\n",
            "Validation Accuracy:  80.0\n",
            "Runtime:  1.7887909531593322  Min\n",
            "Best candidate:  {'lr': 0.001, 'batch_size': 32, 'optimizer': 'adam', 'dropout_rate': 0.5}\n",
            "Testing on the test Set: \n",
            "Test Accuracy on Final MLP Model:  79.34\n"
          ]
        }
      ]
    }
  ]
}